{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "KaggleTuto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "KN4S5fKJlfyT"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#data = pd.read_csv('drive/MyDrive/ColabNotebooks/MPR1/Files/ner_dataset.csv', encoding= 'unicode_escape')\n",
    "data = pd.read_csv('drive/MyDrive/ColabNotebooks/MPR1/Files/kaggle_at_home.csv', encoding= 'unicode_escape')\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')] # remove Unnamed column\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "4Vf9ysr3Ng6t",
    "outputId": "ee1accde-4859-4a6f-a9ff-58b4d276b676"
   },
   "source": [
    "data = data.head(10000)\n",
    "data['Word'].fillna(\" \", inplace=True)\n",
    "data.head()"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FOURTH</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SECTION</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CASE</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #     Word  POS Tag\n",
       "0  Sentence: 1           NNS  O \n",
       "1          NaN   FOURTH  NNS  O \n",
       "2          NaN  SECTION  NNS  O \n",
       "3          NaN           NNS  O \n",
       "4          NaN     CASE  NNS  O "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t2IlWkyFmRRQ"
   },
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GcXUPFg4m4Y5",
    "outputId": "b1d7c386-938f-4549-eb82-649a06ed97ef"
   },
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FOURTH</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SECTION</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CASE</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>209</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #     Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1           NNS  O        117        2\n",
       "1          NaN   FOURTH  NNS  O        165        2\n",
       "2          NaN  SECTION  NNS  O        200        2\n",
       "3          NaN           NNS  O        117        2\n",
       "4          NaN     CASE  NNS  O        209        2"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "4QvzCQrTm6oF",
    "outputId": "82e25b5d-8bdd-479b-8510-ad4aff90ee36"
   },
   "source": [
    "# Fill na\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence #'],as_index=False\n",
    ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "# Visualise data\n",
    "data_group.head()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[ , FOURTH, SECTION,  , CASE, OF, POHOSKA, v.,...</td>\n",
       "      <td>[NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, ...</td>\n",
       "      <td>[O , O , O , O , O , O , O , O , O , O , O , O...</td>\n",
       "      <td>[117, 165, 200, 117, 209, 217, 152, 107, 117, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[ , 4, .]</td>\n",
       "      <td>[NNS, NNS, NNS]</td>\n",
       "      <td>[O , B CARDINAL, O ]</td>\n",
       "      <td>[117, 144, 47]</td>\n",
       "      <td>[2, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 11</td>\n",
       "      <td>[ , On, 30, August, 2010, the, President, of, ...</td>\n",
       "      <td>[NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, ...</td>\n",
       "      <td>[O , O , B DATE, I DATE, I DATE, O , O , O , B...</td>\n",
       "      <td>[117, 98, 105, 122, 35, 123, 199, 137, 123, 14...</td>\n",
       "      <td>[2, 2, 6, 12, 12, 2, 2, 2, 9, 15, 15, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 12</td>\n",
       "      <td>[ , THE, FACTS,  , I.,  , THE, CIRCUMSTANCES, ...</td>\n",
       "      <td>[NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, ...</td>\n",
       "      <td>[O , O , B ORG, O , O , O , O , O , O , O , O ...</td>\n",
       "      <td>[117, 147, 126, 117, 69, 117, 147, 203, 217, 1...</td>\n",
       "      <td>[2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 13</td>\n",
       "      <td>[ , The, applicant, was, born, in, 1951, and, ...</td>\n",
       "      <td>[NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, NNS, ...</td>\n",
       "      <td>[O , O , O , O , O , O , B DATE, O , O , O , O ]</td>\n",
       "      <td>[117, 23, 74, 145, 99, 92, 79, 59, 214, 92, 47]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #  ...                                            Tag_idx\n",
       "0   Sentence: 1  ...  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, ...\n",
       "1  Sentence: 10  ...                                          [2, 0, 2]\n",
       "2  Sentence: 11  ...  [2, 2, 6, 12, 12, 2, 2, 2, 9, 15, 15, 2, 2, 2,...\n",
       "3  Sentence: 12  ...         [2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2]\n",
       "4  Sentence: 13  ...                  [2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2]\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VeSgqai7m9UL"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MBnMWQqnFva",
    "outputId": "26726976-9be6-48cc-9c19-47c8cfaeee3c"
   },
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= n_tag - 1)\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_tokens length: 13 \n",
      "train_tokens length: 13 \n",
      "test_tokens length: 2 \n",
      "test_tags: 2 \n",
      "val_tokens: 5 \n",
      "val_tags: 5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nbHuXhRfnTYc"
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iv5REUmundsa"
   },
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJv8D7Y5nfkn",
    "outputId": "59f4b312-4396-4a57-bd12-2c4a3a51ab7b"
   },
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_dim:  228 \n",
      "output_dim:  64 \n",
      "input_length:  99 \n",
      "n_tags:  16\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VaJIpIy2qRlA"
   },
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HZtua-z9qWW_"
   },
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1zdlVWyqaFy",
    "outputId": "722e8ad3-d1df-4cb2-98d5-3b5ded02679d"
   },
   "source": [
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 99, 64)            14592     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 99, 128)          66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 99, 64)            49408     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 99, 16)           1040      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,088\n",
      "Trainable params: 131,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 10s 10s/step - loss: 14.6711 - accuracy: 0.0000e+00 - val_loss: 1.8900 - val_accuracy: 0.6734\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 1.4595 - accuracy: 0.8182 - val_loss: 1.3532 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.9156 - accuracy: 0.8465 - val_loss: 1.2755 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 0.7381 - accuracy: 0.8404 - val_loss: 1.1835 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 0.6202 - accuracy: 0.8455 - val_loss: 1.1142 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.5316 - accuracy: 0.8384 - val_loss: 1.0638 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.4921 - accuracy: 0.8374 - val_loss: 1.0192 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.4360 - accuracy: 0.8374 - val_loss: 1.0065 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.4053 - accuracy: 0.8374 - val_loss: 1.0070 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.4227 - accuracy: 0.8354 - val_loss: 1.0132 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 0.4182 - accuracy: 0.8374 - val_loss: 1.0182 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.4216 - accuracy: 0.8394 - val_loss: 1.0195 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.4331 - accuracy: 0.8394 - val_loss: 1.0166 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.4413 - accuracy: 0.8374 - val_loss: 1.0101 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.4260 - accuracy: 0.8364 - val_loss: 1.0002 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 0.3874 - accuracy: 0.8434 - val_loss: 0.9882 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.4109 - accuracy: 0.8616 - val_loss: 0.9755 - val_accuracy: 0.6532\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.4072 - accuracy: 0.8636 - val_loss: 0.9625 - val_accuracy: 0.6633\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.4026 - accuracy: 0.8606 - val_loss: 0.9500 - val_accuracy: 0.6768\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 0.3603 - accuracy: 0.8879 - val_loss: 0.9376 - val_accuracy: 0.7037\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 0.3907 - accuracy: 0.8879 - val_loss: 0.9258 - val_accuracy: 0.7273\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.3518 - accuracy: 0.9010 - val_loss: 0.9146 - val_accuracy: 0.7576\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 0.3577 - accuracy: 0.9071 - val_loss: 0.9041 - val_accuracy: 0.7811\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.3433 - accuracy: 0.9162 - val_loss: 0.8942 - val_accuracy: 0.8081\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.3357 - accuracy: 0.9253 - val_loss: 0.8851 - val_accuracy: 0.8350\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7qw2PdWeqcJ1"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}